{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZxZ-D5sjo6j",
    "outputId": "68138d1b-36e7-45c3-fbcc-7f163b12a2a9"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/dayeonki/askqe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixS-7AgUkOnr",
    "outputId": "2f8a3496-0cb9-4078-fba0-378c939f012d"
   },
   "outputs": [],
   "source": [
    "!pip install -q vllm\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp-8YW4ykniy"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "REPO_PATH = \"askqe\"\n",
    "\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "sys.path.insert(0, f\"{REPO_PATH}/QG/code\")\n",
    "sys.path.insert(0, f\"{REPO_PATH}/QA/code\")\n",
    "sys.path.insert(0, f\"{REPO_PATH}/biomqm/askqe\")\n",
    "sys.path.insert(0, f\"{REPO_PATH}/evaluation/string-comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLlzxX9PmXFU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGodi61Zw1lp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xRSABx4kzb2"
   },
   "source": [
    "#Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWluwKGUj5wr"
   },
   "outputs": [],
   "source": [
    "from QG.code.prompt import nli as qg_prompt_template\n",
    "from QA.code.prompt import qa_prompt as qa_prompt_template\n",
    "from biomqm.askqe.prompt import atomic_fact_prompt as atomic_fact_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7oBwxvhmIkq",
    "outputId": "539570d9-187b-4768-cae2-248b941a3ce4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "LIMIT = None # change to minimize data or None\n",
    "\n",
    "\n",
    "def calculate_mqm_score(errors):\n",
    "    weights = {\"No error\": 0, \"Neutral\": 0, \"Minor\": 1, \"Major\": 5, \"Critical\": 25}\n",
    "    score_penalty = 0\n",
    "    for error in errors:\n",
    "        severity = error.get(\"severity\")\n",
    "        score_penalty += weights.get(severity, 0)\n",
    "    return max(0, 100 - score_penalty)\n",
    "\n",
    "def get_max_severity(errors):\n",
    "    if not errors:\n",
    "        return \"No Error\"\n",
    "    severities = [e.get(\"severity\") for e in errors]\n",
    "    if \"Critical\" in severities:\n",
    "        return \"Critical\"\n",
    "    if \"Major\" in severities:\n",
    "        return \"Major\"\n",
    "    return \"Minor\"\n",
    "\n",
    "def load_biomqm_data(file_path, limit=None):\n",
    "    data_entries = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if limit and i >= limit:\n",
    "                    break\n",
    "                item = json.loads(line)\n",
    "\n",
    "                # BIOMQM structure: src, tgt, bt_tgt, errors_tgt\n",
    "                entry = {\n",
    "                    'id': item.get('doc_id', f'doc_{i}'),\n",
    "                    'source': item['src'],\n",
    "                    'target': item['tgt'],\n",
    "                    'backtranslation': item.get('bt_tgt', ''),\n",
    "                    'errors': item.get('errors_tgt', []),\n",
    "                    'mqm_score': calculate_mqm_score(item.get('errors_tgt', [])),\n",
    "                    'severity': get_max_severity(item.get('errors_tgt', []))\n",
    "                }\n",
    "                data_entries.append(entry)\n",
    "\n",
    "        print(f\"Loaded {len(data_entries)} entries from BIOMQM successfully.\")\n",
    "        return data_entries\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "BIOMQM_FILE = f\"{REPO_PATH}/biomqm/dev_with_backtranslation.jsonl\"\n",
    "dataset = load_biomqm_data(BIOMQM_FILE, limit=LIMIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJDnB-fluAtT"
   },
   "source": [
    "#Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZPMKiHwt_5t",
    "outputId": "6218e0ad-36df-4d5d-e24a-ec55daf21cd3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def term_in_data(df):\n",
    "    valid_rows = []\n",
    "\n",
    "    for row in df:\n",
    "        tgt = row.get('target', '')\n",
    "        src = row.get('source', '')\n",
    "        errors = row.get('errors', [])\n",
    "        is_row_clean = True\n",
    "\n",
    "        for error in errors:\n",
    "            term = error.get('term', '')\n",
    "\n",
    "            if term:\n",
    "                if term not in tgt and term not in src:\n",
    "                    is_row_clean = False\n",
    "                    break\n",
    "\n",
    "        if is_row_clean:\n",
    "            valid_rows.append(row)\n",
    "\n",
    "    return valid_rows\n",
    "\n",
    "\n",
    "print(\"SHAPE BEFORE:\", len(dataset))\n",
    "dataset_clean = term_in_data(dataset)\n",
    "print(\"SHAPE AFTER:\", len(dataset_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZzrHbBgmhJE"
   },
   "source": [
    "#Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350,
     "referenced_widgets": [
      "0bd6fab5d7b445e489146bd0d709699f",
      "6b20b30807d04e1e96d5aa04f0d26de6",
      "bfffceb6ba8041629b71cc17ba8c99a3",
      "e611c4e9ba844f55a5ce4eef89961c75",
      "7a00f41f9aa149518b69c7ec5fea4e5f",
      "568f3558a3bd46c1a9cbdd58f0da15c7",
      "8599545cb13645c2a0674909f14449c4",
      "2bb845572e044f16a94b839afd990831",
      "c7f67c1feb8347ed82244d4c2e9b954f",
      "e41b35dabda74551bc93a31220b86dcf",
      "08ad1353ceb343808d4a58e9082e2e3b",
      "ce1e7c6853b54745b41139dd4631f4fa",
      "5b3ab8edc8174ef489d7c0931e116c52",
      "a1a600b41b93430e8b4ad183e38ece68",
      "2b81bec29596445e8126afb978540911",
      "91a4227e261c425a9366591cfa78a960",
      "2fca4a968a844eaab775a47a7a834a16",
      "5f51b8821b42462b8dc41e20cec14abb",
      "dea740fe94074a2ebc200b0de0a23780",
      "61dc177bcbb74f5fa70c0f2dee2c23f1",
      "e2ef0dc784a94b3393a541179ba62057",
      "42f5fe9eebb74cd29d5724c92074a3fd",
      "9989dd05052148428bbdefc9c2633840",
      "c4acb8633e454233a9e6cfc5d9e3c35d",
      "5057659946484a0e97a2bf7e759501f0",
      "2deab758857940a3bfd87281bc69491a",
      "73f0d64ce09b4066af728315baf6c622",
      "f7e978d4c53c46ca8243dbccc860596c",
      "43c91995d0ba4407b0ab0b3f7cd5504c",
      "5d446b23de0049488ba9c1321d677418",
      "b17af9edb37f4ca9b2c1c79e2f96e84b",
      "fea6bbcb86444d359671fcc1489c3b1a",
      "6f61c5fa26e744f9acd04123d6bdd619",
      "569676077e894969a9a16fc28f2a79d5",
      "7196e2725a414d03bf65aabdaf9e99ff",
      "bc0cafcffd0045c68e977fee53e1af84",
      "46f8aa0ff98e4f25b4d3c75d3a301daf",
      "e1cf0afce1324bcda603e5f1b5ce9569",
      "adbd3266adbb4077b67fa084c242151d",
      "0e9cb53ae2e545abaf1e13f854ec9c67",
      "8f9d678b756145dba470adea51ac72ef",
      "3947aedaaea24cc0a63869722ccc10e5",
      "666eb23ca1624888a229b2651045faf6",
      "955d79b5267a4de08e8c99c30387cd23",
      "fb954a2813fc45fbbb1f580ac9b4d208",
      "1c0147b60ac840c08464c06a440eebd5",
      "1dd2560b97c242bd9b22b0ba26215e97",
      "b5760ef8967440ec81c289b199d85020",
      "93492ef268574225afdd80907910f073",
      "81e146beabbb4b9188f819942a8d7258",
      "3926e3b2bb1449438310831f6cf6012b",
      "cc8806357dd04215aa15d866b2674e70",
      "b79f05f76b654e9489eef7e73d2700ce",
      "51008da3e43241c395965de60bc59f3a",
      "632ae1c3a02c4571a666ab71c38ff5f0",
      "f5f6e4840e374cd3a544e6a6e8be8366",
      "1904518ababd4c9a8994a3008a5ccefe",
      "da5eb8d4d2ed49538351ffc9f806a7be",
      "fed1f007cc974355819166d9b034bfe5",
      "73a9e2a0637d43898593e96181e14043",
      "c44c8c6635e94d6ba59bd96a8d1cc380",
      "1b560188798c4836b91ee240f7bd7eb9",
      "b181ea432a004e579c226737134eb6bf",
      "3e1cb09a64694a7fad846c194a4c8d90",
      "72d9ee63938b4ff4baa11a4663f3396a",
      "9206b6139c164a2d9322095c80b485d8"
     ]
    },
    "id": "3FsQeda3miJ1",
    "outputId": "b1038a3d-a46b-498d-bd2c-d282e7bc8663"
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "MODEL_ID = \"01-ai/Yi-1.5-9B-Chat\"\n",
    "\n",
    "# QWEN\n",
    "#\"Qwen/Qwen2.5-7B-Instruct-AWQ\" +\n",
    "#\"Qwen/Qwen2.5-14B-Instruct-AWQ\" +\n",
    "#\"Qwen/Qwen3-4B-Instruct-2507\" +\n",
    "#\"cyankiwi/Qwen3-30B-A3B-Instruct-2507-AWQ-4bit\"\n",
    "\n",
    "\n",
    "# Llama\n",
    "#\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# casperhansen/llama-3-8b-instruct-awq +\n",
    "\n",
    "# Gemma\n",
    "#\"casperhansen/gemma-7b-it-awq\"\n",
    "#\"google/gemma-2-9b-it\" +\n",
    "#\"solidrust/gemma-2-9b-it-AWQ\"\n",
    "\n",
    "# Yi\n",
    "#\"01-ai/Yi-1.5-9B\"\n",
    "#\"01-ai/Yi-1.5-6B\"\n",
    "\n",
    "# Mistral\n",
    "#casperhansen/mistral-nemo-instruct-2407-awq\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    dtype=\"half\",\n",
    "    max_model_len=2048,\n",
    "    gpu_memory_utilization=0.60,\n",
    "    # quantization=\"awq\",\n",
    "    seed=0,\n",
    "    enable_prefix_caching=True,\n",
    "    disable_log_stats=True,\n",
    "    # enforce_eager=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmUun0oUpnu1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "SAMPLING_PARAMS = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "\n",
    "def generate_text_batch(prompts, sampling_params=SAMPLING_PARAMS):\n",
    "    formatted_prompts = []\n",
    "    for p in prompts:\n",
    "        messages = [{\"role\": \"user\", \"content\": p}]\n",
    "        formatted = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        formatted_prompts.append(formatted)\n",
    "\n",
    "    outputs = llm.generate(formatted_prompts, sampling_params)\n",
    "\n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        text = output.outputs[0].text.strip()\n",
    "        if not text.endswith(\"]\"):\n",
    "            text = text + \"]\"\n",
    "        if not text.startswith(\"[\"):\n",
    "            text = \"[\" + text\n",
    "        generated_texts.append(text)\n",
    "\n",
    "    return generated_texts\n",
    "\n",
    "def parse_list_output(text):\n",
    "    \"\"\"Parses a string representation of a list into a Python list.\"\"\"\n",
    "    import ast\n",
    "    try:\n",
    "        start = text.find('[')\n",
    "        end = text.rfind(']') + 1\n",
    "        if start != -1 and end != -1:\n",
    "            candidate = text[start:end]\n",
    "            return ast.literal_eval(candidate)\n",
    "        return []\n",
    "    except:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyH68_3Zs5R7"
   },
   "source": [
    "#AskQE pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871,
     "referenced_widgets": [
      "86716d2c376540f08dc15bba00ddc13b",
      "cf90470500994702baac063c96a11fe6",
      "3b74519d5f0e43859e3c10864f1f9b38",
      "5b2978d33ef74edfafc720d31416b2b7",
      "4f64edebc88041ae805e109b6e8186e4",
      "ed544895bd0b4fdd9758e693081d9af4",
      "b92e46605a444734aaf1d493a39a79b6",
      "02fe6e66ce8b4457b93f4cf6332e27af",
      "dd1c04b716254462af119753cfbac132",
      "5c714f42db314b39a11d0b5d2413de4d",
      "d7d1d8cd377d48ba96d19cd42bf6f22e",
      "4427ce2673d94d4b9c6e27f3be260e4b",
      "bc7abe694013498c8b2fda341d8157db",
      "6d1cb96672c044288c10adaeea946f2f",
      "86a6b7175a23453a8cbad97a3e19a6b9",
      "32d3cd8dcd89476d85c4619abf98dbe8",
      "b07b64955c234089b3d36e6412136746",
      "973ae3698a6c455bb6b36b5c234a0e77",
      "ac54513fd6b74c0895028f4dd4ef84b7",
      "cee26a6bb60a43708d986b6c320f1287",
      "8cd6372fcd82455ebb5cb1ef551d1d6c",
      "cecbfa7d53224d69bd6774d2a1eb102f",
      "ee97e15caa4247f2a0cdcb0109c5cf2f",
      "c5ab06b5e8f7453f949ca18adb6e6177",
      "44339ce739414f879ebbb85d726d9fb9",
      "3f8881a3b49647f0be316ae2b8e08fd5",
      "6fbe9d9d3cf346649014fdce86e180f0",
      "73ab9e82c3924ae1b933eecae7163984",
      "30208102597245aa8c540766edc36ea6",
      "14721d80d5694f649c844c7602002b32",
      "aa967ac3c09e45c4b92f03249f8bc85b",
      "040f2c36d13b4bd489d227da69e4285f",
      "b50e4d71a85740348490e2a3f53608f7",
      "0ef6557007d44f98ab54f39324eb60e7",
      "61e1e25a62eb4df580e38d0c9eed4f6c",
      "fd5a60e93fbc4ed3b040af0da1cc2e98",
      "a9f0ecece5a343e79ee63aa0588cb5e3",
      "d8326c30c2134831b821917c2a903410",
      "6bd0741dc75e41d98a2c3fd0188f5cce",
      "15db25cacb9f432fb27566d24b9494c0",
      "f15a7369295b4507bd8510196d22c293",
      "bfa4a31d8deb4daa95092734dd2d9ac7",
      "db749598b7564b81b25658edeb68e12c",
      "0266583f83654bb99300ff112e4f797f",
      "53b88d4fceca4e9ba79f5261387f4135",
      "725b8a5f99e24292b439f7186c619ba2",
      "88d3b3454da845f08b921bf46923b0df",
      "5dc296f494854afdab616378797aa41b",
      "9839b27e43c9425785f906af57739877",
      "b719a7516d424b4b811009378edf5d87",
      "f737dad8d50d400a9209bd42e2f7446c",
      "76b85eed266849d699fd44805b595bec",
      "a359a1ed45ff402d901904d954d038ec",
      "c712e6e2a4564cce910440e51536deae",
      "6c332ad37dc544aaab6b6187bbd68211",
      "5472d424056147fb95dce6b756220d47",
      "24e9ca0dce8a4a7fbfb9e2b2a029a29a",
      "5676ba66aa454c9eb7a08c9d829fe261",
      "e7aa97ccd5f64839b3dd4d826024b8f1",
      "680740e84012482bac44b1806544f178",
      "227bd5582aca48d79a29b32594624789",
      "a525847893d44728a25996c68194695a",
      "e733831fea1c48b09d281d70a5187285",
      "1c416e5a7eee40b3b0f8dd41315bb33a",
      "facb151c38664aa5abc21955207e4350",
      "8b0df58c28c1496e8717622326891a03"
     ]
    },
    "id": "fCQgTODarHTi",
    "outputId": "2d1f373d-8b63-42ec-b1fc-80110249cbc1"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "N = len(dataset)\n",
    "\n",
    "all_facts = [[] for _ in range(N)]\n",
    "all_questions = [[] for _ in range(N)]\n",
    "all_answers_src = [[] for _ in range(N)]\n",
    "all_answers_bt = [[] for _ in range(N)]\n",
    "\n",
    "print(f\"Using cleaned data with shape = {len(dataset_clean)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 1: Atomic Fact Extraction\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1: Atomic Fact Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompts_facts = [\n",
    "    atomic_fact_prompt_template.replace(\"{{sentence}}\", e[\"source\"])\n",
    "    for e in dataset_clean\n",
    "]\n",
    "\n",
    "facts_str_list = generate_text_batch(prompts_facts, SAMPLING_PARAMS)\n",
    "raw_all_facts = [parse_list_output(s) for s in facts_str_list]\n",
    "print(f\"Facts extracted: {sum(len(f) for f in raw_all_facts)} total facts\")\n",
    "num_empty = raw_all_facts.count([])\n",
    "print(f\"Number of empty fact lists: {num_empty}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 1.5: Entailment Filtering\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1.5: NLI Entailment Filtering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "NLI_MODEL_ID = \"potsawee/deberta-v3-large-mnli\"\n",
    "\n",
    "nli_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=NLI_MODEL_ID,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "flat_nli_inputs = []\n",
    "for idx, facts in enumerate(raw_all_facts):\n",
    "    if not facts:\n",
    "        continue\n",
    "    source = dataset_clean[idx][\"source\"]\n",
    "    for fact in facts:\n",
    "        flat_nli_inputs.append((idx, fact, [source, fact]))\n",
    "\n",
    "print(f\"Running NLI on {len(flat_nli_inputs)} fact-source pairs...\")\n",
    "nli_pairs = [{\"text\": t[2][0], \"text_pair\": t[2][1]} for t in flat_nli_inputs]\n",
    "nli_results = nli_pipeline(nli_pairs, batch_size=64, truncation=True, max_length=512)\n",
    "\n",
    "kept = 0\n",
    "for (idx, fact, sfp), res in zip(flat_nli_inputs, nli_results):\n",
    "    label = res[\"label\"].upper()\n",
    "    score = res[\"score\"]\n",
    "    if (\"LABEL_1\" not in label) or (\"LABEL_1\" in label and score < 0.85):\n",
    "        all_facts[idx].append(fact)\n",
    "        kept += 1\n",
    "\n",
    "removed = sum(len(f) for f in raw_all_facts) - kept\n",
    "print(f\"NLI Complete: {kept} facts retained (filtered {removed} contradictions)\")\n",
    "num_empty = all_facts.count([])\n",
    "print(f\"Number of empty fact lists: {num_empty}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 2: Question Generation\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 2: Question Generation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "jobs = []\n",
    "for idx, facts in enumerate(all_facts):\n",
    "    if facts:\n",
    "        prompt = qg_prompt_template.replace(\"{{sentence}}\", dataset_clean[idx][\"source\"]).replace(\"{{atomic_facts}}\", str(facts))\n",
    "        jobs.append({\"idx\": idx, \"prompt\": prompt})\n",
    "\n",
    "prompts = [j[\"prompt\"] for j in jobs]\n",
    "results = generate_text_batch(prompts, SAMPLING_PARAMS)\n",
    "\n",
    "for job, result in zip(jobs, results):\n",
    "    all_questions[job[\"idx\"]] = parse_list_output(result)\n",
    "\n",
    "print(f\"Questions generated: {sum(len(q) for q in all_questions)} total\")\n",
    "print(f\"Number of empty question lists: {all_questions.count([])}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 3 & 4: QA on Source & Backtranslation\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 3 & 4: Question Answering (src + BT)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompts_qa_src = []\n",
    "prompts_qa_bt = []\n",
    "qa_idx_map = []\n",
    "\n",
    "for idx, questions in enumerate(all_questions):\n",
    "    if not questions:\n",
    "        continue\n",
    "\n",
    "    # scr prompt\n",
    "    p_src = (\n",
    "        qa_prompt_template\n",
    "        .replace(\"{{sentence}}\", dataset_clean[idx][\"source\"])\n",
    "        .replace(\"{{questions}}\", str(questions))\n",
    "    )\n",
    "    prompts_qa_src.append(p_src)\n",
    "\n",
    "    # BT prompt\n",
    "    p_bt = (\n",
    "        qa_prompt_template\n",
    "        .replace(\"{{sentence}}\", dataset_clean[idx].get(\"backtranslation\", \"\"))\n",
    "        .replace(\"{{questions}}\", str(questions))\n",
    "    )\n",
    "    prompts_qa_bt.append(p_bt)\n",
    "\n",
    "    qa_idx_map.append(idx)\n",
    "\n",
    "combined_prompts = prompts_qa_src + prompts_qa_bt\n",
    "all_answers = generate_text_batch(combined_prompts, SAMPLING_PARAMS)\n",
    "\n",
    "split_idx = len(prompts_qa_src)\n",
    "answers_src = all_answers[:split_idx]\n",
    "answers_bt = all_answers[split_idx:]\n",
    "\n",
    "for prompt_pos, dataset_idx in enumerate(qa_idx_map):\n",
    "    curr_questions = all_questions[dataset_idx]\n",
    "\n",
    "    src_ans = parse_list_output(answers_src[prompt_pos])\n",
    "    bt_ans = parse_list_output(answers_bt[prompt_pos])\n",
    "\n",
    "    if len(src_ans) != len(curr_questions) or len(bt_ans) != len(curr_questions):\n",
    "        src_ans = []\n",
    "        bt_ans = []\n",
    "\n",
    "    src_ans = src_ans[:len(curr_questions)]\n",
    "    bt_ans = bt_ans[:len(curr_questions)]\n",
    "\n",
    "    all_answers_src[dataset_idx] = src_ans\n",
    "    all_answers_bt[dataset_idx] = bt_ans\n",
    "\n",
    "print(f\"QA Complete: {sum(len(a) for a in all_answers_src)} source answers, \"\n",
    "      f\"{sum(len(a) for a in all_answers_bt)} BT answers\")\n",
    "print(f\"Number of empty answers lists (SRC): {all_answers_src.count([])}\")\n",
    "print(f\"Number of empty answers lists (BT): {all_answers_bt.count([])}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL: Build Results List\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL: Building Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for idx, entry in enumerate(dataset_clean):\n",
    "    results.append({\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"source\": entry[\"source\"],\n",
    "        \"backtranslation\": entry.get(\"backtranslation\", \"\"),\n",
    "        \"facts\": all_facts[idx],\n",
    "        \"questions\": all_questions[idx],\n",
    "        \"answers_src\": all_answers_src[idx],\n",
    "        \"answers_bt\": all_answers_bt[idx],\n",
    "        \"mqm_score\": entry.get(\"mqm_score\", None),\n",
    "        \"severity\": entry.get(\"severity\", None),\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\\nPipeline execution complete.\")\n",
    "\n",
    "output_file = \"askqe_data.jsonl\"\n",
    "with open(output_file, 'w') as f:\n",
    "    for res in results:\n",
    "        f.write(json.dumps(res) + '\\n')\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHnqswDFiv4a"
   },
   "source": [
    "#Scoring & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfoJmT0Yv7cw",
    "outputId": "c1f2dbf1-9a89-457c-8793-cf1904e6cf53"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "from utils import (\n",
    "    f1_score as repo_f1_score,\n",
    "    exact_match_score as repo_exact_match_score,\n",
    "    chrf_score as repo_chrf_score,\n",
    "    bleu_score as repo_bleu_score,\n",
    "    compare_answers as repo_compare_answers,\n",
    "    normalize_answer\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "output_file = \"askqe_data.jsonl\"\n",
    "\n",
    "results = []\n",
    "lost_cnt = 0\n",
    "with open(output_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        data = json.loads(line)\n",
    "        if data.get(\"answers_src\") and data.get(\"answers_bt\"):\n",
    "            results.append(data)\n",
    "        else:\n",
    "            lost_cnt += 1\n",
    "\n",
    "print(f\"Loaded {len(results)} out of {len(results) + lost_cnt} records from {output_file}\")\n",
    "print(\"First element example:\", results[0] if results else \"empty list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CavlvU6zsN1B"
   },
   "source": [
    "## ASKQE scores using different metrics evaluated on BIOMQM per error severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lUyzQRVo-dQ",
    "outputId": "1f4ece09-4c37-4139-9389-371921af81de"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sacrebleu import sentence_bleu, sentence_chrf\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "print(\"Calculating Scores...\")\n",
    "\n",
    "all_src_answers = []\n",
    "all_bt_answers = []\n",
    "indices_map = []\n",
    "\n",
    "for i, res in enumerate(results):\n",
    "\n",
    "    src_list = res.get('answers_src', [])\n",
    "    bt_list = res.get('answers_bt', [])\n",
    "\n",
    "    if src_list and bt_list:\n",
    "        src_str_list = [str(x) for x in src_list]\n",
    "        bt_str_list = [str(x) for x in bt_list]\n",
    "\n",
    "        all_src_answers.extend(src_str_list)\n",
    "        all_bt_answers.extend(bt_str_list)\n",
    "\n",
    "        indices_map.append((i, len(src_list)))\n",
    "\n",
    "        f1_vals = []\n",
    "        em_vals = []\n",
    "        bleu_vals = []\n",
    "        chrf_vals = []\n",
    "\n",
    "        for p, g in zip(src_str_list, bt_str_list):\n",
    "            f1_vals.append(repo_f1_score(p, g, normalize=True))\n",
    "            em_vals.append(int(repo_exact_match_score(p, g, normalize=True)))\n",
    "            bleu_vals.append(sentence_bleu(p, [g]).score)\n",
    "            chrf_vals.append(sentence_chrf(p, [g]).score)\n",
    "\n",
    "        results[i]['askqe_f1'] = np.mean(f1_vals)\n",
    "        results[i]['askqe_em'] = np.mean(em_vals)\n",
    "        results[i]['askqe_bleu'] = np.mean(bleu_vals)\n",
    "        results[i]['askqe_chrf'] = np.mean(chrf_vals)\n",
    "\n",
    "    else:\n",
    "        indices_map.append((i, 0))\n",
    "        results[i]['askqe_f1'] = 0.0\n",
    "        results[i]['askqe_em'] = 0.0\n",
    "        results[i]['askqe_bleu'] = 0.0\n",
    "        results[i]['askqe_chrf'] = 0.0\n",
    "        results[i]['askqe_sbert'] = 0.0\n",
    "\n",
    "print(f\"Encoding {len(all_src_answers)} answer pairs simultaneously...\")\n",
    "\n",
    "embeddings_src = sbert_model.encode(all_src_answers, batch_size=64, convert_to_tensor=True)\n",
    "embeddings_bt = sbert_model.encode(all_bt_answers, batch_size=64, convert_to_tensor=True)\n",
    "\n",
    "cursor = 0\n",
    "for idx, count in indices_map:\n",
    "    if count > 0:\n",
    "        doc_scores = []\n",
    "        for k in range(count):\n",
    "            sim = util.cos_sim(embeddings_src[cursor + k], embeddings_bt[cursor + k]).item()\n",
    "            doc_scores.append(sim)\n",
    "\n",
    "        results[idx]['askqe_sbert'] = np.mean(doc_scores)\n",
    "        cursor += count\n",
    "    else:\n",
    "        if 'askqe_sbert' not in results[idx]:\n",
    "             results[idx]['askqe_sbert'] = 0.0\n",
    "\n",
    "print(\"Scoring complete\")\n",
    "\n",
    "output_file = \"askqe_results.jsonl\"\n",
    "with open(output_file, 'w') as f:\n",
    "    for res in results:\n",
    "        f.write(json.dumps(res) + '\\n')\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "NxiiS_vflRzd",
    "outputId": "3dd4bf47-038f-43b9-82c0-d4d1a4679006"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "categories = [\"No Error\", \"Minor\", \"Major\", \"Critical\"]\n",
    "print(f\"{'Severity':<10} {'F1':<6} {'EM':<6} {'CHRF':<6} {'BLEU':<6} {'SBERT':<6}\")\n",
    "\n",
    "for cat in categories:\n",
    "\n",
    "    group = [r for r in results if r.get('severity') == cat]\n",
    "    if not group:\n",
    "        continue\n",
    "\n",
    "    avg_f1    = np.mean([r.get('askqe_f1', 0) for r in group])\n",
    "    avg_em    = np.mean([r.get('askqe_em', 0) for r in group])\n",
    "    avg_chrf  = np.mean([r.get('askqe_chrf', 0) for r in group])\n",
    "    avg_bleu  = np.mean([r.get('askqe_bleu', 0) for r in group])\n",
    "    avg_sbert = np.mean([r.get('askqe_sbert', 0) for r in group])\n",
    "\n",
    "    print(f\"{cat:<10} {avg_f1:.3f}  {avg_em:.3f}  {avg_chrf:.2f}   {avg_bleu:.2f}   {avg_sbert:.3f}\")\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "order = [\"No Error\", \"Minor\", \"Major\", \"Critical\"]\n",
    "\n",
    "sns.boxplot(x='severity', y='askqe_sbert', data=pd.DataFrame(results), order=order)\n",
    "plt.title('ASKQE (SBERT) Score by Error Severity')\n",
    "plt.ylabel('ASKQE Score (SBERT)')\n",
    "plt.xlabel('Error Severity')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "sns.boxplot(x='severity', y='askqe_f1', data=pd.DataFrame(results), order=order)\n",
    "plt.title('ASKQE (F1) Score by Error Severity')\n",
    "plt.ylabel('ASKQE Score (F1)')\n",
    "plt.xlabel('Error Severity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZQCGY8SpIbA"
   },
   "source": [
    "###Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6apr5Ejk2wDY",
    "outputId": "858ea9d5-089b-40f4-8e43-0c1bc85447a2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_file = \"askqe_results.jsonl\"\n",
    "\n",
    "results = []\n",
    "with open(output_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(results)} records from {output_file}\")\n",
    "print(\"First element example:\", results[0] if results else \"empty list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "4um96vXknHNd",
    "outputId": "d7eaac32-c670-42d9-af4e-28fbac4176ce"
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Correlation Analysis (Kendall's Tau)\")\n",
    "print(\"Expected: Strong POSITIVE correlation (Higher ASKQE score = Higher MQM score)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "metrics = {\n",
    "    \"SBERT\": \"askqe_sbert\",\n",
    "    \"F1\":    \"askqe_f1\",\n",
    "    \"EM\":    \"askqe_em\",\n",
    "    \"BLEU\":  \"askqe_bleu\",\n",
    "    \"CHRF\":  \"askqe_chrf\"\n",
    "}\n",
    "names = []\n",
    "values = []\n",
    "\n",
    "print(f\"{'Metric':<10} {'Tau':<10} {'P-Value':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for name, col_name in metrics.items():\n",
    "    tau, p_value = scipy.stats.kendalltau(df_results[col_name], df_results['mqm_score'], nan_policy='omit')\n",
    "    names.append(name)\n",
    "    values.append(tau)\n",
    "    print(f\"{name:<10} {tau:.4f}     {p_value:.4e}\")\n",
    "\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(names, values, zorder=3)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5, zorder=0)\n",
    "plt.axhline(0, color='black', linewidth=1)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.3f}',\n",
    "             ha='center', va='bottom' if height > 0 else 'top', fontsize=10)\n",
    "\n",
    "plt.ylabel(\"Kendall's Tau\")\n",
    "plt.title(\"Correlation with Human Judgments (MQM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6MTmaGyp21Q"
   },
   "source": [
    "##BT-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8P8aPSe_Jco3"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(data, filename):\n",
    "    print(f\"   [Checkpoint] Saving {len(data)} records to {filename}...\")\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    print(\"   [Checkpoint] Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mExPSHcE6sAO"
   },
   "outputs": [],
   "source": [
    "!pip install -q bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396,
     "referenced_widgets": [
      "d21495be03944d6989e137e962993d90",
      "005795724e27496faed8cf49ab6296c2",
      "afc53bd94bfa4e4091ec8d429c8787b3",
      "c8827fbbb51f453b94799c0dfa2ffc0f",
      "5c009cb08c77494d9363c633d7fa2880",
      "58ebc4ce5b384f89bbbd48cc3b64fb44",
      "89f54a8cc3c444e8bf8ae608fc8a3acc",
      "7e6bc21aa2b34166926890e4bb408af9",
      "db3b18b399e24529bee8a6a262ff81a5",
      "df9e7b6715d443718008ba37c4f88880",
      "e35fe77f1024442daf90f96ab6c3bf0c",
      "5b3ed1e7d0824c2785ebc6bdebe9f8ba",
      "04ec7e9180824b40a40c83a97c6ac5c1",
      "a026df0e894e44d9bb6bacee7f2fea82",
      "efba974ad52642c39f583fa3791f9d95",
      "aebf7ea2a25840d38037301d7d55b53a",
      "eac535c6450941e098a466425ade485b",
      "d777d629a5564b34bff466cec3450e60",
      "19dd536447154baa8c1c2431c7779a08",
      "6bc64cf227e4430f857048c91a9008dc",
      "68add346ccde423d96b40000327b9517",
      "5b7b2018240b40dba2d0fc57a5800b17"
     ]
    },
    "id": "6dE7mHuavpYU",
    "outputId": "a9aed3c3-5582-40a8-921d-14cda45416ed"
   },
   "outputs": [],
   "source": [
    "from bert_score import score as bert_score\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "print(\"Calculating BT-Score...\")\n",
    "\n",
    "_, _, F1 = bert_score(\n",
    "    [r['backtranslation'] for r in results],\n",
    "    [r['source'] for r in results],\n",
    "    lang=\"en\",\n",
    "    rescale_with_baseline=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "f1_values = F1.numpy()\n",
    "for item, score in zip(results, f1_values):\n",
    "    item['bt_score'] = float(score)\n",
    "\n",
    "for r in results:\n",
    "    if 'bt_score' not in r:\n",
    "        print(\"WARNING. Some data is missing\")\n",
    "        r['bt_score'] = 0.0\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "tau, p_value = scipy.stats.kendalltau(df_results['bt_score'], df_results['mqm_score'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"{'Metric':<10} {'Tau':<10} {'P-Value':<10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'BT-Score':<10} {tau:.4f}     {p_value:.4e}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "save_checkpoint(results, \"askqe_results_final.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47jsKH1u7V9l"
   },
   "source": [
    "## xCOMET-QE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzLl7Vj2ApUm"
   },
   "outputs": [],
   "source": [
    "!pip install -q unbabel-comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "a362c351b9c64119a517e51c55b6d18e",
      "0f3001d03f19496f8cb80212fcf220ea",
      "4768dfa0514647659c6661883c56bc88",
      "ddc4388983a94fb6b693e690faa7a060",
      "ca0178249ac24532a8357a575617b751",
      "730b31603e614683a9dfaa9cc6a844ad",
      "7660ee730d844bd8b0c5c9f5e23053dd",
      "f68bd1ea8acc436e9ac92d3d27625480",
      "cdf981795c3248f391518be794ae22e5",
      "60c07d5e17084629b0ca834809c6b901",
      "6c3ca5b5bcc84b90988ea793e42090ff"
     ]
    },
    "id": "biCefcTq7iII",
    "outputId": "b6d49729-d791-49ba-e5b2-d0fe3fbd80f0"
   },
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "print(\"Loading xCOMET-QE model...\")\n",
    "model_path = download_model(\"Unbabel/wmt20-comet-qe-da\") # Unbabel/wmt22-cometkiwi-da\n",
    "model = load_from_checkpoint(model_path)\n",
    "\n",
    "print(\"Calculating xCOMET-QE Score...\")\n",
    "\n",
    "comet_data = []\n",
    "valid_indices = []\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    if r.get('source') and r.get('backtranslation'):\n",
    "        comet_data.append({\n",
    "            \"src\": r['source'],\n",
    "            \"mt\": r['backtranslation']\n",
    "        })\n",
    "        valid_indices.append(i)\n",
    "    else:\n",
    "        print(\"WARNING. Some data is missing\")\n",
    "        r['xcomet_score'] = 0.0\n",
    "\n",
    "if comet_data:\n",
    "    gpus = 1 if torch.cuda.is_available() else 0\n",
    "    model_output = model.predict(comet_data, batch_size=8, gpus=gpus)\n",
    "    for idx, score in zip(valid_indices, model_output.scores):\n",
    "        results[idx]['xcomet_score'] = float(score)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "tau, p_value = scipy.stats.kendalltau(df_results['xcomet_score'], df_results['mqm_score'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"{'Metric':<10} {'Tau':<10} {'P-Value':<10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'xCOMET-QE':<10} {tau:.4f}     {p_value:.4e}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "save_checkpoint(results, \"askqe_results_final.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjEyFaZCqANH"
   },
   "source": [
    "##Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FyNlCiyNvpa2",
    "outputId": "4afe8609-9312-401a-c9db-e1d6727eeda6"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metrics_to_analyze = {\n",
    "    'ASKQE (F1)':    'askqe_f1',\n",
    "    'ASKQE (SBERT)': 'askqe_sbert',\n",
    "    'ASKQE (BLEU)':  'askqe_bleu',\n",
    "    'ASKQE (CHRF)':  'askqe_chrf',\n",
    "    'ASKQE (EM)':    'askqe_em',\n",
    "    'xCOMET-QE':     'xcomet_score'\n",
    "}\n",
    "\n",
    "plot_config = [\n",
    "    ('ASKQE (F1)',    'askqe_f1',    'blue'),\n",
    "    ('ASKQE (SBERT)', 'askqe_sbert', 'green'),\n",
    "    ('ASKQE (BLEU)',  'askqe_bleu',  'purple'),\n",
    "    ('ASKQE (CHRF)',  'askqe_chrf',  'orange'),\n",
    "    ('ASKQE (EM)',    'askqe_em',    'red'),\n",
    "    ('xCOMET-QE',     'xcomet_score','brown')\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pearson Correlation Analysis: Metrics vs BT-Score\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for label, col in metrics_to_analyze.items():\n",
    "\n",
    "    if col == 'xcomet_score':\n",
    "        continue\n",
    "\n",
    "    corr, p_val = pearsonr(df_results[col], df_results['bt_score'])\n",
    "    sig = \"Significant\" if p_val < 0.05 else \"Not Significant\"\n",
    "\n",
    "    print(f\"\\n{label} vs BT-Score:\")\n",
    "    print(f\"  Pearson r = {corr:.4f}\")\n",
    "    print(f\"  p-value   = {p_val:.4e} ({sig})\")\n",
    "\n",
    "plot_config_fig1 = [item for item in plot_config if item[1] != 'xcomet_score']\n",
    "\n",
    "fig1, axes1 = plt.subplots(2, 3, figsize=(18, 10))\n",
    "if isinstance(axes1, np.ndarray): axes1 = axes1.flatten()\n",
    "\n",
    "for i, (label, col, color) in enumerate(plot_config_fig1):\n",
    "    ax = axes1[i]\n",
    "\n",
    "    x_data = df_results[col]\n",
    "\n",
    "    ax.scatter(x_data, df_results['bt_score'], alpha=0.5, s=30, color=color)\n",
    "\n",
    "    z = np.polyfit(df_results[col], df_results['bt_score'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(df_results[col].min(), df_results[col].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'r--', alpha=0.8, label='Linear fit')\n",
    "\n",
    "    corr, _ = pearsonr(df_results[col], df_results['bt_score'])\n",
    "    ax.set_title(f'{label} vs BT-Score\\nPearson r = {corr:.3f}')\n",
    "\n",
    "    ax.set_xlabel(label)\n",
    "    ax.set_ylabel('BT-Score')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "for j in range(len(plot_config_fig1), len(axes1)):\n",
    "    fig1.delaxes(axes1[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Correlation: Metrics vs BT-Score', y=1.02, fontsize=16)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Pearson Correlation Analysis: ASKQE Metrics vs xCOMET-QE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "askqe_metrics_only = {k: v for k, v in metrics_to_analyze.items() if v != 'xcomet_score'}\n",
    "\n",
    "for label, col in askqe_metrics_only.items():\n",
    "\n",
    "    corr, p_val = pearsonr(df_results[col], df_results['xcomet_score'])\n",
    "\n",
    "    sig = \"Significant\" if p_val < 0.05 else \"Not Significant\"\n",
    "    print(f\"\\n{label} vs xCOMET-QE:\")\n",
    "    print(f\"  Pearson r = {corr:.4f}\")\n",
    "    print(f\"  p-value   = {p_val:.4e} ({sig})\")\n",
    "\n",
    "plot_config_fig2 = plot_config_fig1\n",
    "\n",
    "fig2, axes2 = plt.subplots(2, 3, figsize=(18, 10))\n",
    "if isinstance(axes2, np.ndarray): axes2 = axes2.flatten()\n",
    "\n",
    "for i, (label, col, color) in enumerate(plot_config_fig2):\n",
    "    ax = axes2[i]\n",
    "\n",
    "    x_data = df_results[col]\n",
    "\n",
    "    ax.scatter(x_data, df_results['xcomet_score'], alpha=0.5, s=30, color=color)\n",
    "\n",
    "    z = np.polyfit(df_results[col], df_results['xcomet_score'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(df_results[col].min(), df_results[col].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), 'k--', alpha=0.8, label='Linear fit')\n",
    "\n",
    "    corr, _ = pearsonr(df_results[col], df_results['xcomet_score'])\n",
    "    ax.set_title(f'{label} vs xCOMET-QE\\nPearson r = {corr:.3f}')\n",
    "\n",
    "    ax.set_xlabel(label)\n",
    "    ax.set_ylabel('xCOMET-QE Score')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "for j in range(len(plot_config_fig2), len(axes2)):\n",
    "    fig2.delaxes(axes2[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Correlation: ASKQE Metrics vs xCOMET-QE', y=1.02, fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6U6yJfTqD2v"
   },
   "source": [
    "##GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QZ5b4Yx6vpdD",
    "outputId": "c5783912-a223-439d-8a03-cf97ce776aff"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def get_human_labels(df):\n",
    "    \"\"\"Generates ground truth booleans (Accept=True) from severity strings.\"\"\"\n",
    "    valid_severities = {'no error', 'neutral', 'minor', 'none'}\n",
    "    return df['severity'].astype(str).str.lower().isin(valid_severities)\n",
    "\n",
    "def fit_gmm(data):\n",
    "    \"\"\"Fits a 2-component GMM and returns the model, means, and cluster assignments.\"\"\"\n",
    "    X = data.values.reshape(-1, 1)\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42).fit(X)\n",
    "\n",
    "    means = gmm.means_.flatten()\n",
    "    accept_idx = np.argmax(means)  # The cluster with higher score is \"Accept\"\n",
    "    labels = gmm.predict(X)\n",
    "\n",
    "    return means, accept_idx, labels\n",
    "\n",
    "def evaluate_metric(df, col, name):\n",
    "    \"\"\"Calculates decision accuracy for a single metric.\"\"\"\n",
    "\n",
    "    # Get GMM predictions\n",
    "    means, accept_idx, labels = fit_gmm(df[col])\n",
    "    pred_accept = (labels == accept_idx)\n",
    "\n",
    "    # Compare with ground truth\n",
    "    human_accept = get_human_labels(df)\n",
    "    accuracy = np.mean(pred_accept == human_accept) * 100\n",
    "\n",
    "    print(f\"{name}: {accuracy:.2f}% (Acc μ: {means[accept_idx]:.3f}, Rej μ: {means[1-accept_idx]:.3f})\")\n",
    "    return accuracy\n",
    "\n",
    "def plot_clusters(df, metrics):\n",
    "    \"\"\"Visualizes GMM clusters for all valid metrics.\"\"\"\n",
    "    valid_metrics = [(c, n) for c, n in metrics if c in df.columns]\n",
    "\n",
    "    n = len(valid_metrics)\n",
    "    cols = 2\n",
    "    rows = (n + 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 4 * rows))\n",
    "    axes = np.array(axes).flatten()\n",
    "\n",
    "    for ax, (col, name) in zip(axes, valid_metrics):\n",
    "        X = df[col].values\n",
    "\n",
    "        means, accept_idx, labels = fit_gmm(df[col])\n",
    "        reject_idx = 1 - accept_idx\n",
    "\n",
    "        # Plot histograms for both clusters\n",
    "        for idx, color, label in [(accept_idx, '#2ecc71', 'Accept'), (reject_idx, '#e74c3c', 'Reject')]:\n",
    "            subset = X[labels == idx]\n",
    "            ax.hist(subset, bins=20, alpha=0.7, color=color, label=f'{label} (μ={means[idx]:.2f})')\n",
    "            ax.axvline(means[idx], color=color, linestyle='--', linewidth=2)\n",
    "\n",
    "        ax.set_title(f'GMM Clustering: {name}')\n",
    "        ax.set_xlabel('Score')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    # Clean up empty subplots\n",
    "    for ax in axes[n:]: fig.delaxes(ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('GMM Decision Boundaries', y=1.02, fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "metrics = [\n",
    "    ('askqe_f1', 'F1'), ('askqe_bleu', 'BLEURT')\n",
    "    ,('askqe_chrf', 'CHRF'), ('askqe_em', 'EM')\n",
    "    ,('askqe_sbert', 'SBERT')\n",
    "    ,('bt_score', 'BERTScore'), ('xcomet_score', 'COMET')\n",
    "]\n",
    "\n",
    "print(\"\\n--- GMM Decision Accuracy ---\")\n",
    "results = {name: evaluate_metric(df_results, col, name) for col, name in metrics}\n",
    "\n",
    "plot_clusters(df_results, metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
